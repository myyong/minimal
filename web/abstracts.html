<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>NIPS 2016 - Artificial Intelligence for Data Science</title>

    <link rel="stylesheet" href="stylesheets/my_styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>NIPS 2016</h1>
        <p>Saturday 10 December 2016
        <br>@ Room 114</p>
        <p class="view"><a href="./index.html">Home</a></p>
        <p class="view"><a href="./schedule.html">Schedule</a></p>
        <p class="view"><a href="./submission.html">Presenters and Papers</a></p>
        <p class="view"><a href="./abstracts.html">Invited Speakers' Abstracts</a></p>
      </header>
      <section>
        <h1>Artificial Intelligence for Data Science</h1>
        <h2>Invited Speakers' Abstracts</h2>
        
        <div class="abstractTitle-box">
        	<a id="Dietterich"></a>
        	<h3>Automated Data Cleaning via Multi-View Anomaly Detection</h3>
        	<h4>Tom Dietterich - Oregon State University <br>
        	<em>9:10 10th December 2016</em> </h4>
        </div>

		<p>One of the first steps in the data analysis pipeline is data cleaning:
			detecting data from failed sensors. This talk will discuss the
			application of anomaly detection algorithms to find and remove bad
			readings from weather station data. We will review our previous work
			on DBN time series models and our current work on applying
			non-parametric anomaly detection algorithms as part of our SENSOR-DX
			multi-view anomaly detection architecture.  A major challenge in
			evaluating these algorithms is to obtain ground truth, because real
			sensor data tends to be labeled conservatively by domain experts.
		</p>
		
		<div class="abstractTitle-box">
        	<a id="Steinruecken"></a>
        	<h3>Automated Model Construction and The Automated Statistician</h3>
        	<h4>Christian Steinruecken - University of Cambridge<br>
        	<em>11:00 10th December 2016</em> </h4>
        </div>

		<p>Abstract
		</p>
		
        <div class="abstractTitle-box">
        	<a id="Guestrin"></a>
        	<h3>Why should I trust you? Explaining the predictions of machine-learning models</h3>
        	<h4>Carlos Guestrin - University of Washington <br>
        	<em>14:00 10th December 2016</em> </h4>
        </div>

		<p>Despite widespread adoption, machine-learning models remain mostly black 
		boxes, making it very difficult to understand the reasons behind a 
		prediction. Such understanding is fundamentally important to assess 
		trust in a model before we take actions based on a prediction or choose 
		to deploy a new ML service. Understand the reasons behind predictions 
		further provides insights into the model, which can be used to turn an 
		untrustworthy model or prediction into a trustworthy one.</p>
		
		<p>In this talk, we a novel explanation technique that explains the 
		predictions of any classifier in an interpretable and faithful manner by 
		learning an interpretable model locally around the prediction, as well 
		as a method to explain models by presenting representative individual 
		predictions and their explanations in a nonredundant way. We demonstrate 
		the flexibility of these methods by explaining different models for text 
		(e.g., random forests) and image classification (e.g., deep neural 
		networks) and explore the usefulness of explanations via novel 
		experiments, with human subjects. These explanations empower users in 
		various scenarios that require trust, such as deciding if one should 
		trust a prediction, choosing between models, improving an untrustworthy 
		classifier, and detecting why a classifier should not be trusted.
		</p>
		
		<div class="abstractTitle-box">
        	<a id="Hutter"></a>
        	<h3>Advances and Challenges in Automated Machine Learning: Blackbox Optimization and Beyond</h3>
        	<h4>Frank Hutter - University of Freiburg<br>
        	<em>15:30 10th December 2016</em> </h4>
        </div>

		<p>
		How do we select which machine learning model to use for a given dataset,
		with which pre- and post-processing steps, and with which hyperparameter
		setting?
		</p>
		<p>
		In this talk, I will first review how Bayesian optimization can be used to
		tackle these problems as a joint blackbox optimization problem, thereby
		enabling automated machine learning. Then, I will discuss extensions of
		Bayesian optimization that go beyond this traditional blackbox formulation
		to effectively attack problems where evaluating a single hyperparameter
		setting can require as long as a week.
		</p>
      </section>
      <footer>
      	<p>Organizers can be contacted at <a href="mailto:ai4datasci@gmail.com" target="_top">ai4datasci at gmail.com</a></p>
        <p><small>Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
