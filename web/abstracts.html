<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>NIPS 2016 - Artificial Intelligence for Data Science</title>

    <link rel="stylesheet" href="stylesheets/my_styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>NIPS 2016</h1>
        <p>Saturday 10 December 2016
        <br>Barcelona Spain</p>
        <p class="view"><a href="./index.html">Home</a></p>
        <p class="view"><a href="./schedule.html">Schedule</a></p>
        <p class="view"><a href="./submission.html">Presenters and Papers</a></p>
        <p class="view"><a href="./abstracts.html">Invited Speakers' Abstracts</a></p>
      </header>
      <section>
        <h1>Artificial Intelligence for Data Science</h1>
        <h2>Invited Speakers' Abstracts</h2>
        
        <!-- <div class="abstractTitle-box">
        	<a id="Dietterich"></a>
        	<h3>Title</h3>
        	<h4>Tim Diettrich - Oregon State University <br>
        	<em>9:10 10th December 2016</em> </h4>
        </div>

		<p>Abstract
		</p> -->
		
		<!-- <div class="abstractTitle-box">
        	<a id="Steinruecken"></a>
        	<h3>Title</h3>
        	<h4>Christian Steinruecken - University of Cambridge<br>
        	<em>9:10 10th December 2016</em> </h4>
        </div>

		<p>Abstract
		</p> -->
		
        <div class="abstractTitle-box">
        	<a id="Guestrin"></a>
        	<h3>Why should I trust you? Explaining the predictions of machine-learning models</h3>
        	<h4>Carlos Guestrin - University of Washington <br>
        	<em>14:00 10th December 2016</em> </h4>
        </div>

		<p>Despite widespread adoption, machine-learning models remain mostly black 
		boxes, making it very difficult to understand the reasons behind a 
		prediction. Such understanding is fundamentally important to assess 
		trust in a model before we take actions based on a prediction or choose 
		to deploy a new ML service. Understand the reasons behind predictions 
		further provides insights into the model, which can be used to turn an 
		untrustworthy model or prediction into a trustworthy one.</p>
		
		<p>In this talk, we a novel explanation technique that explains the 
		predictions of any classifier in an interpretable and faithful manner by 
		learning an interpretable model locally around the prediction, as well 
		as a method to explain models by presenting representative individual 
		predictions and their explanations in a nonredundant way. We demonstrate 
		the flexibility of these methods by explaining different models for text 
		(e.g., random forests) and image classification (e.g., deep neural 
		networks) and explore the usefulness of explanations via novel 
		experiments, with human subjects. These explanations empower users in 
		various scenarios that require trust, such as deciding if one should 
		trust a prediction, choosing between models, improving an untrustworthy 
		classifier, and detecting why a classifier should not be trusted.
		</p>
		
		<!-- <div class="abstractTitle-box">
        	<a id="Hutter"></a>
        	<h3>Title</h3>
        	<h4>Frank Hutter - University of Freiburg<br>
        	<em>15:30 10th December 2016</em> </h4>
        </div>

		<p>Abstract
		</p> -->
      </section>
      <footer>
      	<p>Organizers can be contacted at <a href="mailto:ai4datasci@gmail.com" target="_top">ai4datasci at gmail.com</a></p>
        <p><small>Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
